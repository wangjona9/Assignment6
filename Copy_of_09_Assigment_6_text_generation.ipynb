{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangjona9/Assignment6/blob/main/Copy_of_09_Assigment_6_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import math\n",
        "\n",
        "import random\n",
        "\n",
        "def extract_sonnets(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    start_index = None\n",
        "    end_index = None\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"THE SONNETS\" in line.upper():\n",
        "            start_index = i + 1\n",
        "        elif \"*** END OF THE PROJECT GUTENBERG EBOOK\" in line:\n",
        "            end_index = i\n",
        "            break\n",
        "\n",
        "    if start_index is None or end_index is None:\n",
        "        raise ValueError(\"Could not find the start or end of the sonnets.\")\n",
        "\n",
        "    # extract only the sonnet text\n",
        "    sonnet_lines = [line.strip() for line in lines[start_index:end_index] if line.strip()]\n",
        "\n",
        "    # remove titles\n",
        "    sonnet_lines = [line for line in sonnet_lines if not line.upper().startswith(\"SONNET\")]\n",
        "\n",
        "    return sonnet_lines\n",
        "\n",
        "def split_data(lines, train_ratio=0.8):\n",
        "\n",
        "    total_lines = len(lines)\n",
        "    train_cutoff = int(total_lines * train_ratio)\n",
        "\n",
        "    train_lines = lines[:train_cutoff]\n",
        "    val_lines = lines[train_cutoff:]\n",
        "\n",
        "    return \"\\n\".join(train_lines), \"\\n\".join(val_lines)\n",
        "\n",
        "input_path = 'sonnets.txt'\n",
        "\n",
        "sonnet_lines = extract_sonnets(input_path)\n",
        "train_text, val_text = split_data(sonnet_lines)\n",
        "\n",
        "with open('sonnets_train.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(train_text)\n",
        "\n",
        "with open('sonnets_val.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(val_text)\n",
        "\n",
        "print(\"Split into sonnets_train.txt and sonnets_val.txt\")"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f57b8e7-0e20-4f66-f84b-dacc1e4fffae"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into sonnets_train.txt and sonnets_val.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\.\\!\\?]', '', text)\n",
        "\n",
        "    # Tokenize by words (split on whitespace)\n",
        "    tokens = text.split()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def build_vocab(tokens):\n",
        "    # Building a vocabulary\n",
        "    word_counts = Counter(tokens)\n",
        "    sorted_words = sorted(word_counts.keys())  # Sort\n",
        "\n",
        "    word_to_id = {word: idx for idx, word in enumerate(sorted_words)}\n",
        "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "with open('combined_sonnets.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Preprocess\n",
        "tokens = preprocess_text(text)\n",
        "\n",
        "# Build vocabulary\n",
        "word_to_id, id_to_word = build_vocab(tokens)\n",
        "\n",
        "# Example outputs\n",
        "print(\"Sample tokens:\", tokens[:20])\n",
        "print(\"Vocab size:\", len(word_to_id))\n",
        "print(\"Sample word_to_id:\", dict(list(word_to_id.items())[:10]))"
      ],
      "metadata": {
        "id": "RvXRFVcbMLe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1e8513-4fc7-4f17-d9de-566521eba8c6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample tokens: ['by', 'william', 'shakespeare', 'i', 'from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that', 'thereby', 'beautys', 'rose', 'might', 'never', 'die', 'but', 'as', 'the']\n",
            "Vocab size: 3667\n",
            "Sample word_to_id: {'a': 0, 'abhor': 1, 'abide': 2, 'able': 3, 'about': 4, 'above': 5, 'absence': 6, 'absence!': 7, 'absent': 8, 'abundance': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "sequence_length = 20\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "OXCK40l6MRld"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    embedding_layer,\n",
        "    Conv1D(filters=256, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(vocab_size, activation='softmax') # Predict next word\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "linweGaUMg0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaf7bf3-d43d-4164-9c22-cd8bb30372f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 65ms/step - accuracy: 0.0224 - loss: 7.2610 - val_accuracy: 0.0215 - val_loss: 6.8979\n",
            "Epoch 2/5\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.0277 - loss: 6.5251 - val_accuracy: 0.0215 - val_loss: 7.0403\n",
            "Epoch 3/5\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.0280 - loss: 6.5063 - val_accuracy: 0.0215 - val_loss: 7.0906\n",
            "Epoch 4/5\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 61ms/step - accuracy: 0.0284 - loss: 6.4951 - val_accuracy: 0.0164 - val_loss: 7.1396\n",
            "Epoch 5/5\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.0294 - loss: 6.4418 - val_accuracy: 0.0249 - val_loss: 7.1873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PerplexityCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        train_perplexity = math.exp(logs['loss'])\n",
        "        val_perplexity = math.exp(logs['val_loss'])\n",
        "        print(f'\\nEpoch {epoch+1}:')\n",
        "        print(f'Train Perplexity: {train_perplexity:.2f}')\n",
        "        print(f'Val Perplexity: {val_perplexity:.2f}')\n",
        "\n",
        "# Train the model with perplexity monitoring\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[PerplexityCallback()]\n",
        ")\n",
        "\n",
        "# Final evaluation\n",
        "final_train_loss = history.history['loss'][-1]\n",
        "final_val_loss = history.history['val_loss'][-1]\n",
        "print(f'\\nFinal Training Perplexity: {math.exp(final_train_loss):.2f}')\n",
        "print(f'Final Validation Perplexity: {math.exp(final_val_loss):.2f}')"
      ],
      "metadata": {
        "id": "P8d8FS2XMj46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2ff15e-6b85-4123-c107-53d5ebfbdcf8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0406 - loss: 6.3319\n",
            "Epoch 1:\n",
            "Train Perplexity: 573.29\n",
            "Val Perplexity: 1374.63\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 60ms/step - accuracy: 0.0406 - loss: 6.3321 - val_accuracy: 0.0232 - val_loss: 7.2259\n",
            "Epoch 2/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0517 - loss: 6.1613\n",
            "Epoch 2:\n",
            "Train Perplexity: 491.05\n",
            "Val Perplexity: 1401.83\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 64ms/step - accuracy: 0.0516 - loss: 6.1616 - val_accuracy: 0.0227 - val_loss: 7.2455\n",
            "Epoch 3/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0659 - loss: 5.9394\n",
            "Epoch 3:\n",
            "Train Perplexity: 396.66\n",
            "Val Perplexity: 1482.36\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 61ms/step - accuracy: 0.0659 - loss: 5.9397 - val_accuracy: 0.0260 - val_loss: 7.3014\n",
            "Epoch 4/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0755 - loss: 5.6729\n",
            "Epoch 4:\n",
            "Train Perplexity: 302.36\n",
            "Val Perplexity: 1583.80\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 59ms/step - accuracy: 0.0754 - loss: 5.6732 - val_accuracy: 0.0221 - val_loss: 7.3676\n",
            "Epoch 5/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0942 - loss: 5.3391\n",
            "Epoch 5:\n",
            "Train Perplexity: 216.01\n",
            "Val Perplexity: 1830.08\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 61ms/step - accuracy: 0.0941 - loss: 5.3394 - val_accuracy: 0.0187 - val_loss: 7.5121\n",
            "Epoch 6/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1117 - loss: 4.9111\n",
            "Epoch 6:\n",
            "Train Perplexity: 143.22\n",
            "Val Perplexity: 2227.56\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 66ms/step - accuracy: 0.1116 - loss: 4.9115 - val_accuracy: 0.0181 - val_loss: 7.7087\n",
            "Epoch 7/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1270 - loss: 4.4246\n",
            "Epoch 7:\n",
            "Train Perplexity: 88.35\n",
            "Val Perplexity: 2966.33\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 61ms/step - accuracy: 0.1269 - loss: 4.4250 - val_accuracy: 0.0266 - val_loss: 7.9951\n",
            "Epoch 8/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1428 - loss: 3.8578\n",
            "Epoch 8:\n",
            "Train Perplexity: 52.28\n",
            "Val Perplexity: 4267.86\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 61ms/step - accuracy: 0.1427 - loss: 3.8586 - val_accuracy: 0.0227 - val_loss: 8.3589\n",
            "Epoch 9/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1841 - loss: 3.3287\n",
            "Epoch 9:\n",
            "Train Perplexity: 32.25\n",
            "Val Perplexity: 6103.28\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 60ms/step - accuracy: 0.1839 - loss: 3.3299 - val_accuracy: 0.0221 - val_loss: 8.7166\n",
            "Epoch 10/10\n",
            "\u001b[1m248/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2380 - loss: 2.9014\n",
            "Epoch 10:\n",
            "Train Perplexity: 21.65\n",
            "Val Perplexity: 8280.53\n",
            "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 65ms/step - accuracy: 0.2376 - loss: 2.9028 - val_accuracy: 0.0204 - val_loss: 9.0217\n",
            "\n",
            "Final Training Perplexity: 21.65\n",
            "Final Validation Perplexity: 8280.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_phrase, num_tokens=50, temperature=0.7):\n",
        "    generated = seed_phrase.lower().split()\n",
        "    for _ in range(num_tokens):\n",
        "        # Convert last 'sequence_length' tokens to IDs\n",
        "        seed_ids = [word_to_id.get(word, 0) for word in generated[-sequence_length:]]\n",
        "        seed_ids = pad_sequences([seed_ids], maxlen=sequence_length)\n",
        "\n",
        "        # Predict next token probabilities\n",
        "        preds = model.predict(seed_ids, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature sampling\n",
        "        preds = np.log(preds) / temperature\n",
        "        exp_preds = np.exp(preds)\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Sample next token\n",
        "        next_id = np.random.choice(len(preds), p=preds)\n",
        "        generated.append(id_to_word[next_id])\n",
        "\n",
        "    return ' '.join(generated)\n",
        "\n",
        "# Generate two distinct samples\n",
        "print(generate_text(seed_phrase=\"thou art\", num_tokens=50))\n",
        "print(generate_text(seed_phrase=\"had stol’n\", num_tokens=50))"
      ],
      "metadata": {
        "id": "1uHjn6aHMW5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110b988a-21b6-4f63-e4c1-4d4318e5d318"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thou art gone which for is beauty my doth doth yet with kind face part to of love what blind those loves was love time sin love that that me me but that it i which from should on for my cannot the for the my for the day forbear of far\n",
            "had stol’n fear souls weep days now form and now the which of worth weep peace now not time it due was his that beautys of now since first could the the wide mend shows hath hath my made whilst on to call sometime belovd from thence they but but those i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5CpdqF9MoPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}